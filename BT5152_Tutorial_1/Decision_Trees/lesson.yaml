- Class: meta
  Course: BT5152 Tutorial 1
  Lesson: Decision Trees
  Author: Wei Lu
  Type: Standard
  Organization: National University of Singapore
  Version: 2.4.3

- Class: text
  Output: In this lesson, we'll learn how to build decision tree based classification models using the training dataset.

- Class: cmd_question
  Output: "First, we start with fitting a simple classification tree model using the package `C50`, which we've already installed for you. Type the command to import the `C50` package:"
  CorrectAnswer: library(C50)
  AnswerTests: omnitest(correctExpr='library(C50)')
  Hint: Try library(C50)

- Class: cmd_question
  Output: "Next, with the same iris_train and iris_test data we have already prepared for you, let's fit a model using the function `C5.0` from the C50 package. Feel free to look at the help documentation of function `C5.0` to find out what are the appropriate arguments required. Assigned the fitted model to variable named `model_tree`:"
  CorrectAnswer: model_tree <- C5.0(Species ~., data=iris_train)
  AnswerTests: any_of_exprs('model_tree <- C5.0(select(iris_train, -Species), iris_train$Species)', 'model_tree <- C5.0(iris_train %>% select(-Species), iris_train$Species)', 'model_tree <- C5.0(Species ~., data=iris_train)')
  Hint: Note that there are two acceptable syntax for function `C5.0` â€“ the first argument can either be a formula, or it needs to be a dataframe of features without the class variable Species. If you want to use the second syntax, you can exclude the column with `select(iris_train, -Species)`. Also, remember to assign the returned model to variable named `model_tree`

- Class: cmd_question
  Output: "We can peek into the tree model. Type summary(model_tree)"
  CorrectAnswer: summary(model_tree)
  AnswerTests: omnitest(correctExpr='summary(model_tree)')
  Hint: Type summary(model_tree)

- Class: cmd_question
  Output: "Alternatively, we can also view the tree model graphically. Type plot(model_tree)"
  CorrectAnswer: plot(model_tree)
  AnswerTests: omnitest(correctExpr='plot(model_tree)')
  Hint: Type plot(model_tree)

- Class: cmd_question
  Output: "Now let's put our tree model to use on our iris_test data and store the predictions in variable named `test_pred_tree`"
  CorrectAnswer: test_pred_tree <- predict(model_tree, select(iris_test, -Species))
  AnswerTests: any_of_exprs('test_pred_tree <- predict(model_tree, select(iris_test, -Species))', 'test_pred_tree <- predict(model_tree, iris_test)', 'test_pred_tree <- predict(model_tree, iris_test %>% select(-Species))')
  Hint: Try predict(model_tree, select(iris_test, -Species)). Remember to assign the returned value to `test_pred_tree`

- Class: cmd_question
  Output: "Calculate the accuracy using the function `mean`, by comparing `test_pred_tree` to `iris_test$Species`:"
  CorrectAnswer: mean(test_pred_tree == iris_test$Species)
  AnswerTests: omnitest(correctExpr='mean(test_pred_tree == iris_test$Species)')
  Hint: Apply `mean` function to the result of `test_pred_tree == iris_test$Species`

- Class: text
  Output: "`C5.0` also supports simple boosting through the argument `trials`, and pre-pruning through the argument `control` where you can specify the minCase, e.g. `control = C5.0Control(minCases = 10)`. Adjusting neither parameters makes a big difference in our case as our tree model is very simple, and both our training and test accuracies are already quite high."

- Class: cmd_question
  Output: "Next, let's build another tree-based classification model using rpart, which implements recursive partitioning and maximizes the Gini index as the default criterion. Type the command to import the `rpart` library:"
  CorrectAnswer: library(rpart)
  AnswerTests: omnitest(correctExpr='library(rpart)')
  Hint: Try library(rpart)

- Class: cmd_question
  Output: "To visualize the tree, we will also need to rpart.plot package. Type the command to import the `rpart.plot` library:"
  CorrectAnswer: library(rpart.plot)
  AnswerTests: omnitest(correctExpr='library(rpart.plot)')
  Hint: Try library(rpart.plot)

- Class: cmd_question
  Output: "Let's fit a model on our iris_train data using the function `rpart`. Assigned the fitted model to variable named `model_rpart`:"
  CorrectAnswer: model_rpart <- rpart(Species ~., data=iris_train)
  AnswerTests: omnitest(correctExpr='model_rpart <- rpart(Species ~., data=iris_train)')
  Hint: Note that the first argument to function `rpart` is a formula, try `Species ~.`

- Class: cmd_question
  Output: "Let's visualized the fitted tree model using `rpart.plot`:"
  CorrectAnswer: rpart.plot(model_rpart)
  AnswerTests: omnitest(correctExpr='rpart.plot(model_rpart)')
  Hint: Type rpart.plot(model_rpart)

- Class: cmd_question
  Output: "Now let's put our tree model to use on our iris_test data and store the predictions in variable named `test_pred_rpart`. Note that you will need to specify type='class' when invoking `predict` (see ?predict.rpart)"
  CorrectAnswer: test_pred_rpart <- predict(model_rpart, select(iris_test, -Species), type='class')
  AnswerTests: any_of_exprs("test_pred_rpart <- predict(model_rpart, select(iris_test, -Species), type='class')", "test_pred_rpart <- predict(model_rpart, iris_test, type='class')", "test_pred_rpart <- predict(model_rpart, iris_test %>% select(-Species), type='class')")
  Hint: Try predict(model_rpart, select(iris_test, -Species), type='class'). Remember to assign the returned value to `test_pred_rpart`

- Class: cmd_question
  Output: "Calculate the accuracy using the function `mean`, by comparing `test_pred_rpart` to `iris_test$Species`:"
  CorrectAnswer: mean(test_pred_rpart == iris_test$Species)
  AnswerTests: omnitest(correctExpr='mean(test_pred_rpart == iris_test$Species)')
  Hint: Apply `mean` function to the result of `test_pred_rpart == iris_test$Species`

- Class: figure
  Output: "You can adjust the tree's size and complexity through tuning the pruning parameters such as minsplit, maxdepth, cp etc. Here are some examples. Observe how each parameter changes the tree structure:"
  Figure: rpartPreprune.R
  FigureType: new

- Class: cmd_question
  Output: "How do we choose which parameters to tune? cp is a powerful parameter that is easy to tune. Let's look at the `cptable` attribute on our `model_rpart`:"
  CorrectAnswer: model_rpart$cptable
  AnswerTests: omnitest(correctExpr='model_rpart$cptable')
  Hint: Type model_rpart$cptable

- Class: text
  Output: "Observe that as the number of splits (nsplit) increases, CP decreases and the training error (rel error) decreases; while the cross validation error (xerror) stops decreasing at nsplit=2. It means that nsplit=2 is a good stopping point to avoid overfitting our model. We can use the corresponding CP value as the parameter for rpart.control when we build an rpart model."

- Class: cmd_question
  Output: "Find the best cp by making use of the minimum value of model_rpart$cptable[,'xerror'], and assign the returned value to variable `best_cp`:"
  CorrectAnswer: best_cp <- model_rpart$cptable[which.min(model_rpart$cptable[,'xerror']), 'CP']
  AnswerTests: omnitest(correctExpr="best_cp <- model_rpart$cptable[which.min(model_rpart$cptable[,'xerror']),'CP']")
  Hint: model_rpart is a matrix, you can use which.min(model_rpart$cptable[,'xerror']) to find the index of the row with the smallest xerror, then use model_rpart$cptable[row_index, 'CP'] to return the best cp. Remember to assign the returned value to variable `best_cp`

- Class: cmd_question
  Output: "Now, build a new rpart model with `best_cp`. Assign the returned model to variable `model_rpart_pruned`"
  CorrectAnswer: model_rpart_pruned <- rpart(Species ~., data=iris_train, control=rpart.control(cp=best_cp))
  AnswerTests: omnitest(correctExpr='model_rpart_pruned <- rpart(Species ~., data=iris_train, control=rpart.control(cp=best_cp))')
  Hint: Invoke `rpart` with parameter `control = rpart.control(cp=best_cp)`

- Class: figure
  Output: "Compare the new model with our baseline model built with the default cp, where cp=0.01:"
  Figure: rpartPostprune.R
  FigureType: new
