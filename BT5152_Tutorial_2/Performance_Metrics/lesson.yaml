- Class: meta
  Course: BT5152_Tutorial_2
  Lesson: Performance Metrics
  Author: Wei Lu
  Type: Standard
  Organization: National University of Singapore
  Version: 2.4.3

- Class: text
  Output: "In this lesson, we'll learn various ways to construct different metrics to measure classification models."

- Class: cmd_question
  Output: "We are going to make use of the dataset BreastCancer from the package mlbench. We've already prepared the training and test datasets. They are stored in `train` and `test` respectively. Have a look at the test dataset: type test"
  CorrectAnswer: test
  AnswerTests: omnitest(correctExpr='test')
  Hint: test

- Class: cmd_question
  Output: "The target variable is Class, whose value is either benign or malignant. We have already trained a decision tree model with C5.0 using the training dataset. The model is saved in `model_c50`. To have a quick view of the classifier, type summary(model_c50)"
  CorrectAnswer: summary(model_c50)
  AnswerTests: omnitest(correctExpr='summary(model_c50)')
  Hint: summary(model_c50)

- Class: cmd_question
  Output: "Use `model_c50` to predict the Class of the `test` dataset, and store the predictions in `predictions_class`. You may find the help doc for predict.C5.0 helpful."
  CorrectAnswer: predictions_class <- predict(model_c50, test)
  AnswerTests: any_of_exprs('predictions_class <- predict(model_c50, select(test, -Class))', 'predictions_class <- predict(model_c50, test)', 'predictions_class <- predict(model_c50, test %>% select(-Class))')
  Hint: predictions <- predict(model_c50, test)

- Class: cmd_question
  Output: "Next, let's build a confusion matrix using the actual test Class and the predicted test Class. The `table` function returns a contingency table of counts at each combination of factor levels. Use `table`, `predictions_class` and `test` to create a confusion matrix with rows representing the predicted class and columns representing the actual class."
  CorrectAnswer: table(predictions_class, test$Class)
  AnswerTests: omnitest(correctExpr='table(predictions_class, test$Class)')
  Hint: table(predictions_class, test$Class)

- Class: cmd_question
  Output: "Now, store the above confusion table to a variable named `cm`"
  CorrectAnswer: cm <- table(predictions_class, test$Class)
  AnswerTests: omnitest(correctExpr='cm <- table(predictions_class, test$Class)')
  Hint: cm <- table(predictions_class, test$Class)

- Class: cmd_question
  Output: "We can access the counts from the returned results of `table` like a matrix, and manually calculate metrics we are interested in, such as sensitivity and specificity. In our case, since we are interested in diagnosing breast cancer, malignant => positive and benign => negative. Calculate sensitivity of our classifier using `cm`:"
  CorrectAnswer: cm[2, 2] / (cm[1, 2] + cm[2, 2])
  AnswerTests: omnitest(correctVal='0.98')
  Hint: cm[2, 2] / (cm[1, 2] + cm[2, 2])

- Class: cmd_question
  Output: "Wouldn't be nice if there's a package that can output not only the counts of actual & predicted classes, but also the percentages? That would save us the trouble of calculating the metrics manually. There is. Let's import the gmodels package"
  CorrectAnswer: library(gmodels)
  AnswerTests: omnitest(correctExpr='library(gmodels)')
  Hint: library(gmodels)

- Class: cmd_question
  Output: "Invoke function `CrossTable` with the same two arguments as you used to invoke `table`, and an additional argument `prop.chisq=F`:"
  CorrectAnswer: CrossTable(predictions_class, test$Class, prop.chisq=F)
  AnswerTests: any_of_exprs('CrossTable(predictions_class, test$Class,  prop.chisq=F)', 'CrossTable(predictions_class, test$Class,  prop.chisq=FALSE)')
  Hint: CrossTable(predictions_class, test$Class, prop.chisq=F)

- Class: text
  Output: "Can you read sensitivity, specificity and false positive rate off the above outputdirectly? You may find this link helpful: https://stats.idre.ucla.edu/spss/faq/how-do-i-interpret-the-results-from-crosstabs/"

- Class: cmd_question
  Output: "Are there magic functions we can invoke which straightaway will return the metrics we are interested in? Introducing the all-powerful caret package. Import the caret package:"
  CorrectAnswer: library(caret)
  AnswerTests: omnitest(correctExpr='library(caret)')
  Hint: library(caret)

- Class: cmd_question
  Output: "Invoke the `sensitivity` function with the confusion matrix `cm` and an appropriate string value for `positive=`"
  CorrectAnswer: sensitivity(cm, positive='malignant')
  AnswerTests: omnitest(correctExpr="sensitivity(cm, positive='malignant')")
  Hint: sensitivity(cm, positive='malignant')

- Class: cmd_question
  Output: "Similarly, invoke the `specificity` function with the confusion matrix `cm` and an appropriate string value for `negative=`"
  CorrectAnswer: specificity(cm, negative='benign')
  AnswerTests: omnitest(correctExpr="specificity(cm, negative='benign')")
  Hint: specificity(cm, negative='benign')

- Class: cmd_question
  Output: "Moreover, caret also provides us functions to calculate precision, recall and F1-score. They are `precision`, `recall` and `F_meas` respectively. Invoke `F_meas` with `cm` and an appropriate string value for `relevant=`:"
  CorrectAnswer: F_meas(cm, relevant='malignant')
  AnswerTests: omnitest(correctExpr="F_meas(cm, relevant='malignant')")
  Hint: F_meas(cm, relevant='malignant')

- Class: cmd_question
  Output: "Verify that the above F1-score is correct by manually calculating the F1-score using `precision(cm, relevant='malignant')` and `recall(cm, relevant='malignant')`"
  CorrectAnswer: 2 * precision(cm, relevant='malignant') * recall(cm, relevant='malignant') / (precision(cm, relevant='malignant') + recall(cm, relevant='malignant'))
  AnswerTests: omnitest(correctExpr="2 * precision(cm, relevant='malignant') * recall(cm, relevant='malignant') / (precision(cm, relevant='malignant') + recall(cm, relevant='malignant'))")
  Hint: 2 * precision(cm, relevant='malignant') * recall(cm, relevant='malignant') / (precision(cm, relevant='malignant') + recall(cm, relevant='malignant'))

- Class: cmd_question
  Output: "We can calculate the kappa statistics by passing `cm` to the built-in function `kappa`, and get the attribute with `$coef` from the returned result"
  CorrectAnswer: kappa(cm)$coef
  AnswerTests: omnitest(correctExpr="kappa(cm)$coef")
  Hint: kappa(cm)$coef

- Class: cmd_question
  Output: "Next, let's plot the receiver operating characteristic (ROC) curve and calculate the area under the curve (AUC) for our classifier. Package ROCR provides us with the functions we need. Import the library `ROCR`:"
  CorrectAnswer: library(ROCR)
  AnswerTests: omnitest(correctExpr='library(ROCR)')
  Hint: library(ROCR)

- Class: cmd_question
  Output: "We will need predicted probabilities of the malignant Class on the `test` dataset, using our model `model_c50`. Invoke function `predict` as before, but with an additional argument `type='prob'`"
  CorrectAnswer: predict(model_c50, test, type='prob')
  AnswerTests: omnitest(correctExpr="predict(model_c50, test, type='prob')")
  Hint: predict(model_c50, test, type='prob')

- Class: cmd_question
  Output: "Notice that the returned matrix contains the probabilities for both benign and malignant classes for each sample. Since we are only interested in the malignant class, select the second column of the above matrix and assign the returned value to `predictions_prob`"
  CorrectAnswer: predictions_prob <- predict(model_c50, test, type='prob')[, 2]
  AnswerTests: omnitest(correctExpr="predictions_prob <- predict(model_c50, test, type='prob')[, 2]")
  Hint: predictions_prob <- predict(model_c50, test, type='prob')[, 2]

- Class: cmd_question
  Output: "Now we are ready to construct a prediction object using the `prediction` functionfrom the ROCR package. Make use of `test$Class` and `predictions_prob` we created above and store the returned value in `pred`"
  CorrectAnswer: pred <- prediction(predictions_prob, labels=test$Class)
  AnswerTests: any_of_exprs("pred <- prediction(predictions_prob, labels=test$Class)", "pred <- prediction(predictions_prob, test$Class)", "pred <- prediction(predictions=predictions_prob, labels=test$Class)")
  Hint: pred <- prediction(predictions_prob, labels=test$Class)

- Class: cmd_question
  Output: "Finally, we can pass the `pred` object to ROCR's `performance` function to create a plottable object. Be sure to specify the performance measures for the ROC curve, namely 'tpr' and 'fpr'. Store the returned object in `roc`:"
  CorrectAnswer: roc <- performance(pred, 'tpr', 'fpr')
  AnswerTests: any_of_exprs("roc <- performance(pred, 'tpr', 'fpr')", "roc <- performance(pred, measure='tpr', x.measure='fpr')")
  Hint: roc <- performance(pred, 'tpr', 'fpr')

- Class: cmd_question
  Output: "Plot `roc`:"
  CorrectAnswer: plot(roc)
  AnswerTests: omnitest(correctExpr="plot(roc)")
  Hint: plot(roc)

- Class: cmd_question
  Output: "Our ROC curve looks more like a v-shape than a curve because our decision tree model is very simple â€“ there are only a handful of possible probability values. You can see this by typing unique(predictions_prob)"
  CorrectAnswer: unique(predictions_prob)
  AnswerTests: omnitest(correctExpr="unique(predictions_prob)")
  Hint: unique(predictions_prob)

- Class: cmd_question
  Output: "We've built another classifer using naiveBayes from package e1071, and already constructed the plottable roc object for you. It's stored in the variable named `roc_nb`. You will see a more curve-like roc curve if you plot it:"
  CorrectAnswer: plot(roc_nb)
  AnswerTests: omnitest(correctExpr="plot(roc_nb)")
  Hint: plot(roc_nb)

- Class: cmd_question
  Output: "You can find the area under the curve (AUC) by invoking the `performance` function with `measure='auc'`. Type performance(pred, 'auc')@y.values"
  CorrectAnswer: performance(pred, 'auc')@y.values
  AnswerTests: omnitest(correctExpr="performance(pred, 'auc')@y.values")
  Hint: performance(pred, 'auc')@y.values
